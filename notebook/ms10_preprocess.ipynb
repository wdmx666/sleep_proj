{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio\n",
    "import joblib\n",
    "import schema\n",
    "import traceback\n",
    "import sklearn\n",
    "import path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as sp_types\n",
    "from pyspark.sql import functions as F\n",
    "from sklearn import preprocessing\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"msg10\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\environment\\python\\anaconda352\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据说明：\n",
    "1. 对于PSG数据\n",
    "   - stage 睡眠所处阶段\n",
    "   - 3 -> 醒来， 2 -> 浅睡， 1 -> 深睡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root=r\"E:\\datasources\\MS10Data\"\n",
    "p = r\"E:\\datasources\\MS10Data\\1\\psg\\psg.mat\"\n",
    "rdp=r\"E:\\datasources\\MS10Data\\1\\raw\\rawMat.mat\"\n",
    "save_root=r\"E:\\datasources\\MS10DataTF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "psg_data_paths=[os.path.join(data_root,f\"{i}\\\\psg\\\\psg.mat\") for i in range(1,33)]\n",
    "raw_data_paths=[os.path.join(data_root,f\"{i}\\\\raw\\\\rawMat.mat\") for i in range(1,33)]\n",
    "psg_raw_data_paths=list(zip(psg_data_paths,raw_data_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mat_psg(fp):\n",
    "    psg1=sio.loadmat(fp)\n",
    "    ti=np.apply_along_axis(lambda row:row[0],0,psg1[\"psgStage\"]['Time']).flatten()\n",
    "    st=np.apply_along_axis(lambda row:row[0],0,psg1[\"psgStage\"]['stage']).flatten()\n",
    "    st=np.apply_along_axis(lambda x:int(x),1,st.reshape(-1,1))\n",
    "    print(st.shape)\n",
    "    result_df = pd.DataFrame(np.vstack([ti,st]).transpose(),columns=['time','stage'])\n",
    "    result_df.loc[:,'idx']=result_df.index\n",
    "    return result_df\n",
    "\n",
    "def parse_mat_signal(fp):\n",
    "    rs=sio.loadmat(fp)\n",
    "    r1=rs['rawData']['dataSignalOld'][0,0]\n",
    "    r2=rs['rawData']['dataSignal'][0,0]\n",
    "    r3=rs['rawData']['dataSwitch'][0,0]\n",
    "    r4=rs['rawData']['Time'][0,0]\n",
    "    result_df = pd.DataFrame(np.hstack([r1,r2,r3,r4]),columns=['data_signal_old','data_signal','data_switch','time'])\n",
    "    result_df.loc[:,'idx']=result_df.index\n",
    "    return result_df\n",
    "\n",
    "def parse_mat_time(matlab_datenum):\n",
    "    return datetime.fromordinal(int(matlab_datenum)) + timedelta(days=matlab_datenum%1) - timedelta(days = 366)\n",
    "\n",
    "\n",
    "def data_to_parquet(file_path,save_root,flag='signal'):\n",
    "    status = False\n",
    "    flag=schema.Or('signal','psg').validate(flag)\n",
    "    save_file_path=os.path.join(save_root,\"ms10_{0}_{1:>03s}.parquet\".format(flag,path.Path(file_path).splitall()[-3]))\n",
    "    print(save_file_path)\n",
    "    try:\n",
    "        if flag=='signal':\n",
    "            parse_mat_signal(file_path).to_parquet(save_file_path, compression='gzip')\n",
    "        else:\n",
    "            parse_mat_psg(file_path).to_parquet(save_file_path, compression='gzip')\n",
    "        status=True\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "    return status\n",
    "\n",
    "mp=lambda it:(data_to_parquet(it[0],save_root=save_root,flag='psg'),data_to_parquet(it[1],save_root=save_root,flag='signal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# with joblib.Parallel(n_jobs=10, prefer=\"threads\") as parallel:\n",
    "#     parallel(joblib.delayed(mp)(it) for it in psg_raw_data_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据解析生成tf-record\n",
    "# 定义特征对象\n",
    "def _int64_feature(value):\n",
    "    \"\"\"生成一个对应类型的特征对象\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"生成一个对应类型的特征对象\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"生成一个对应类型的特征对象\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _make_named_features(named_features):\n",
    "    \"\"\"生成一个命名的特征，给特征对象赋予一个schema\"\"\"\n",
    "    return tf.train.Features(feature=named_features)\n",
    "\n",
    "# 定义序列record，多了一层包裹的抽象\n",
    "def _feature_list(feature_list):\n",
    "    \"\"\"生成一个特征对象的序列\"\"\"\n",
    "    return tf.train.FeatureList(feature=feature_list)\n",
    "\n",
    "def _make_named_featurelists(named_featurelists):\n",
    "    \"\"\"生成一个命名的特征，给特征对象的序列赋予一个schema\"\"\"\n",
    "    return tf.train.FeatureLists(feature_list=named_featurelists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+---+\n",
      "|             time|stage|idx|\n",
      "+-----------------+-----+---+\n",
      "|737236.9168981481|  3.0|  0|\n",
      "|737236.9172453685|  3.0|  1|\n",
      "|737236.9175925888|  3.0|  2|\n",
      "|737236.9179398092|  3.0|  3|\n",
      "|737236.9182870295|  3.0|  4|\n",
      "|737236.9186342498|  3.0|  5|\n",
      "|737236.9189814702|  3.0|  6|\n",
      "|737236.9193286905|  3.0|  7|\n",
      "|737236.9196759108|  3.0|  8|\n",
      "|737236.9200231312|  3.0|  9|\n",
      "|737236.9203703515|  3.0| 10|\n",
      "|737236.9207175719|  3.0| 11|\n",
      "|737236.9210647922|  3.0| 12|\n",
      "|737236.9214120125|  3.0| 13|\n",
      "|737236.9217592329|  3.0| 14|\n",
      "|737236.9221064532|  3.0| 15|\n",
      "|737236.9224536736|  3.0| 16|\n",
      "|737236.9228008939|  3.0| 17|\n",
      "|737236.9231481142|  3.0| 18|\n",
      "|737236.9234953346|  3.0| 19|\n",
      "+-----------------+-----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "psg_sdf=spark.read.parquet(r\"E:\\datasources\\MS10DataTF\\ms10_psg_014.parquet\")\n",
    "psg_sdf=psg_sdf.drop(F.col(\"__index_level_0__\"))\n",
    "psg_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-----------+-----------------+---+\n",
      "|data_signal_old|data_signal|data_switch|             time|idx|\n",
      "+---------------+-----------+-----------+-----------------+---+\n",
      "|        10884.0|     8416.8|        1.0|737236.7675810185|  0|\n",
      "|        11072.0|     8454.4|        1.0|737236.7675810637|  1|\n",
      "|        11196.0|     8479.2|        1.0| 737236.767581109|  2|\n",
      "|        11260.0|     8492.0|        1.0|737236.7675811541|  3|\n",
      "|        11328.0|     8505.6|        1.0|737236.7675811993|  4|\n",
      "|        11336.0|     8507.2|        1.0|737236.7675812446|  5|\n",
      "|        11364.0|     8512.8|        1.0|737236.7675812898|  6|\n",
      "|        11392.0|     8518.4|        1.0| 737236.767581335|  7|\n",
      "|        11356.0|     8511.2|        1.0|737236.7675813802|  8|\n",
      "|        11352.0|     8510.4|        1.0|737236.7675814254|  9|\n",
      "|        11344.0|     8508.8|        1.0|737236.7675814707| 10|\n",
      "|        11340.0|     8508.0|        1.0|737236.7675815158| 11|\n",
      "|        11356.0|     8511.2|        1.0| 737236.767581561| 12|\n",
      "|        11356.0|     8511.2|        1.0|737236.7675816063| 13|\n",
      "|        11412.0|     8522.4|        1.0|737236.7675816515| 14|\n",
      "|        11480.0|     8536.0|        1.0|737236.7675816966| 15|\n",
      "|        11540.0|     8548.0|        1.0|737236.7675817419| 16|\n",
      "|        11628.0|     8565.6|        1.0|737236.7675817871| 17|\n",
      "|        11708.0|     8581.6|        1.0|737236.7675818322| 18|\n",
      "|        11832.0|     8606.4|        1.0|737236.7675818775| 19|\n",
      "+---------------+-----------+-----------+-----------------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "signal_sdf=spark.read.parquet(r\"E:\\datasources\\MS10DataTF\\ms10_signal_014.parquet\")\n",
    "signal_sdf=signal_sdf.drop(F.col(\"__index_level_0__\"))\n",
    "signal_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql import types as sp_types\n",
    "from sklearn import preprocessing\n",
    "scale_f=F.pandas_udf(lambda s:pd.Series(preprocessing.scale(s)),returnType=sp_types.FloatType(),functionType=F.PandasUDFType.SCALAR)\n",
    "\n",
    "scaler= preprocessing.StandardScaler()\n",
    "\n",
    "@F.pandas_udf(returnType=sp_types.FloatType(),functionType=F.PandasUDFType.SCALAR)\n",
    "def scale_f2(s):\n",
    "    rs=scaler.fit_transform(s.values.reshape(-1,1))\n",
    "    print(f\"s.size-> {s.size}\")\n",
    "    return pd.Series(rs.flatten())\n",
    "    \n",
    "\n",
    "two_fold_f=F.udf(lambda value: value*2,returnType=sp_types.FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data_signal_old</th>\n",
       "      <th>data_signal</th>\n",
       "      <th>data_switch</th>\n",
       "      <th>time</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10884.0</td>\n",
       "      <td>8416.8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>737236.767581</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11072.0</td>\n",
       "      <td>8454.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>737236.767581</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11196.0</td>\n",
       "      <td>8479.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>737236.767581</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11260.0</td>\n",
       "      <td>8492.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>737236.767581</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11328.0</td>\n",
       "      <td>8505.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>737236.767581</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data_signal_old  data_signal  data_switch           time  idx\n",
       "0          10884.0       8416.8          1.0  737236.767581    0\n",
       "1          11072.0       8454.4          1.0  737236.767581    1\n",
       "2          11196.0       8479.2          1.0  737236.767581    2\n",
       "3          11260.0       8492.0          1.0  737236.767581    3\n",
       "4          11328.0       8505.6          1.0  737236.767581    4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf=pd.read_parquet(r\"E:\\datasources\\MS10DataTF\\ms10_signal_014.parquet\")\n",
    "pdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.8865163 ],\n",
       "       [ 0.94245236],\n",
       "       [ 0.97934635],\n",
       "       ...,\n",
       "       [-0.0132211 ],\n",
       "       [-0.01084084],\n",
       "       [-0.0132211 ]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler=preprocessing.StandardScaler()\n",
    "scaler.fit_transform(pdf['data_signal'].values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([7820.88717012]), array([672.19613375]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_,scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-----------+-----------------+---+--------------------+\n",
      "|data_signal_old|data_signal|data_switch|             time|idx|scale_data_signal601|\n",
      "+---------------+-----------+-----------+-----------------+---+--------------------+\n",
      "|        10884.0|     8416.8|        1.0|737236.7675810185|  0|           2.1085901|\n",
      "|        11072.0|     8454.4|        1.0|737236.7675810637|  1|            2.244437|\n",
      "|        11196.0|     8479.2|        1.0| 737236.767581109|  2|           2.3340383|\n",
      "|        11260.0|     8492.0|        1.0|737236.7675811541|  3|           2.3802838|\n",
      "|        11328.0|     8505.6|        1.0|737236.7675811993|  4|             2.42942|\n",
      "|        11336.0|     8507.2|        1.0|737236.7675812446|  5|           2.4352007|\n",
      "|        11364.0|     8512.8|        1.0|737236.7675812898|  6|           2.4554331|\n",
      "|        11392.0|     8518.4|        1.0| 737236.767581335|  7|           2.4756658|\n",
      "|        11356.0|     8511.2|        1.0|737236.7675813802|  8|           2.4496524|\n",
      "|        11352.0|     8510.4|        1.0|737236.7675814254|  9|            2.446762|\n",
      "|        11344.0|     8508.8|        1.0|737236.7675814707| 10|           2.4409814|\n",
      "|        11340.0|     8508.0|        1.0|737236.7675815158| 11|            2.438091|\n",
      "|        11356.0|     8511.2|        1.0| 737236.767581561| 12|           2.4496524|\n",
      "|        11356.0|     8511.2|        1.0|737236.7675816063| 13|           2.4496524|\n",
      "|        11412.0|     8522.4|        1.0|737236.7675816515| 14|           2.4901175|\n",
      "|        11480.0|     8536.0|        1.0|737236.7675816966| 15|           2.5392537|\n",
      "|        11540.0|     8548.0|        1.0|737236.7675817419| 16|            2.582609|\n",
      "|        11628.0|     8565.6|        1.0|737236.7675817871| 17|           2.6461968|\n",
      "|        11708.0|     8581.6|        1.0|737236.7675818322| 18|            2.704004|\n",
      "|        11832.0|     8606.4|        1.0|737236.7675818775| 19|            2.793605|\n",
      "+---------------+-----------+-----------+-----------------+---+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dc3=signal_sdf.withColumn(\"scale_data_signal601\",scale_f2(F.col('data_signal')))\n",
    "dc3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14817280"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dc3.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=pd.Series(np.arange(10))\n",
    "rs2=ps.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1=signal_sdf.agg(F.mean(F.col('data_signal'))).first().asDict()['avg(data_signal)']\n",
    "std1=signal_sdf.agg(F.stddev(F.col('data_signal'))).first().asDict()['stddev_samp(data_signal)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc4=signal_sdf.withColumn(\"scale_data_signal61\",(F.col('data_signal')-m1)/std1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-----------+-----------------+---+-------------------+\n",
      "|data_signal_old|data_signal|data_switch|             time|idx|scale_data_signal61|\n",
      "+---------------+-----------+-----------+-----------------+---+-------------------+\n",
      "|        10884.0|     8416.8|        1.0|737236.7675810185|  0| 0.8865162708447009|\n",
      "|        11072.0|     8454.4|        1.0|737236.7675810637|  1| 0.9424523240939083|\n",
      "|        11196.0|     8479.2|        1.0| 737236.767581109|  2| 0.9793463166625358|\n",
      "|        11260.0|     8492.0|        1.0|737236.7675811541|  3| 0.9983883773431158|\n",
      "|        11328.0|     8505.6|        1.0|737236.7675811993|  4| 1.0186205668162336|\n",
      "|        11336.0|     8507.2|        1.0|737236.7675812446|  5|  1.021000824401307|\n",
      "|        11364.0|     8512.8|        1.0|737236.7675812898|  6| 1.0293317259490589|\n",
      "|        11392.0|     8518.4|        1.0| 737236.767581335|  7| 1.0376626274968137|\n",
      "|        11356.0|     8511.2|        1.0|737236.7675813802|  8| 1.0269514683639884|\n",
      "|        11352.0|     8510.4|        1.0|737236.7675814254|  9| 1.0257613395714504|\n",
      "|        11344.0|     8508.8|        1.0|737236.7675814707| 10| 1.0233810819863773|\n",
      "|        11340.0|     8508.0|        1.0|737236.7675815158| 11|  1.022190953193842|\n",
      "|        11356.0|     8511.2|        1.0| 737236.767581561| 12| 1.0269514683639884|\n",
      "|        11356.0|     8511.2|        1.0|737236.7675816063| 13| 1.0269514683639884|\n",
      "|        11412.0|     8522.4|        1.0|737236.7675816515| 14| 1.0436132714594952|\n",
      "|        11480.0|     8536.0|        1.0|737236.7675816966| 15| 1.0638454609326131|\n",
      "|        11540.0|     8548.0|        1.0|737236.7675817419| 16|  1.081697392820658|\n",
      "|        11628.0|     8565.6|        1.0|737236.7675817871| 17| 1.1078802262564575|\n",
      "|        11708.0|     8581.6|        1.0|737236.7675818322| 18| 1.1316828021071839|\n",
      "|        11832.0|     8606.4|        1.0|737236.7675818775| 19| 1.1685767946758086|\n",
      "+---------------+-----------+-----------+-----------------+---+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dc4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(avg(scale_data_signal61)=-5.308867562187707e-12)\n",
      "Row(stddev_samp(scale_data_signal61)=0.999999999999934)\n"
     ]
    }
   ],
   "source": [
    "print(dc4.agg(F.mean(F.col('scale_data_signal61'))).first())\n",
    "print(dc4.agg(F.stddev(F.col('scale_data_signal61'))).first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\environment\\python\\anaconda352\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int32 was converted to float64 by the scale function.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0   -1.566699\n",
       "1   -1.218544\n",
       "2   -0.870388\n",
       "3   -0.522233\n",
       "4   -0.174078\n",
       "5    0.174078\n",
       "6    0.522233\n",
       "7    0.870388\n",
       "8    1.218544\n",
       "9    1.566699\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.arange(10))\n",
    "pd.Series(preprocessing.scale(pd.Series(np.arange(10))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagg=signal_sdf.agg(F.mean(F.col('data_signal')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(avg(data_signal)=7820.887170125228)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagg.first()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc=signal_sdf.withColumn(\"scale_data_signal\",scale_f(F.col('data_signal')))\n",
    "dcg=dc.groupBy(F.col('data_switch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "exprs should not be empty",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-7aa0b7fcf750>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdcg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\environment\\python\\anaconda352\\lib\\site-packages\\pyspark\\sql\\group.py\u001b[0m in \u001b[0;36magg\u001b[1;34m(self, *exprs)\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Alice'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mu'Bob'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         \"\"\"\n\u001b[1;32m---> 86\u001b[1;33m         \u001b[1;32massert\u001b[0m \u001b[0mexprs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"exprs should not be empty\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jgd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexprs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: exprs should not be empty"
     ]
    }
   ],
   "source": [
    "dcg.agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_sdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_udf(\"integer\", PandasUDFType.SCALAR)  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#signal_sdf.filter(signal_sdf['time']==psg_sdf['time']).show()\n",
    "signal_sdf.filter(signal_sdf.time.between(737236.7675813802,737236.7675814707)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "df = spark.createDataFrame([Row(r=Row(a=1, b=\"b\"))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1=Row(a=1, b=\"b\")\n",
    "df.r.getField('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([([1, 2], {\"key\": \"value\"})], [\"l\", \"d\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "left.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-----------+-----------------+---+--------+\n",
      "|data_signal_old|data_signal|data_switch|             time|idx|features|\n",
      "+---------------+-----------+-----------+-----------------+---+--------+\n",
      "|        10884.0|     8416.8|        1.0|737236.7675810185|  0|[8416.8]|\n",
      "|        11072.0|     8454.4|        1.0|737236.7675810637|  1|[8454.4]|\n",
      "|        11196.0|     8479.2|        1.0| 737236.767581109|  2|[8479.2]|\n",
      "|        11260.0|     8492.0|        1.0|737236.7675811541|  3|[8492.0]|\n",
      "|        11328.0|     8505.6|        1.0|737236.7675811993|  4|[8505.6]|\n",
      "|        11336.0|     8507.2|        1.0|737236.7675812446|  5|[8507.2]|\n",
      "|        11364.0|     8512.8|        1.0|737236.7675812898|  6|[8512.8]|\n",
      "|        11392.0|     8518.4|        1.0| 737236.767581335|  7|[8518.4]|\n",
      "|        11356.0|     8511.2|        1.0|737236.7675813802|  8|[8511.2]|\n",
      "|        11352.0|     8510.4|        1.0|737236.7675814254|  9|[8510.4]|\n",
      "|        11344.0|     8508.8|        1.0|737236.7675814707| 10|[8508.8]|\n",
      "|        11340.0|     8508.0|        1.0|737236.7675815158| 11|[8508.0]|\n",
      "|        11356.0|     8511.2|        1.0| 737236.767581561| 12|[8511.2]|\n",
      "|        11356.0|     8511.2|        1.0|737236.7675816063| 13|[8511.2]|\n",
      "|        11412.0|     8522.4|        1.0|737236.7675816515| 14|[8522.4]|\n",
      "|        11480.0|     8536.0|        1.0|737236.7675816966| 15|[8536.0]|\n",
      "|        11540.0|     8548.0|        1.0|737236.7675817419| 16|[8548.0]|\n",
      "|        11628.0|     8565.6|        1.0|737236.7675817871| 17|[8565.6]|\n",
      "|        11708.0|     8581.6|        1.0|737236.7675818322| 18|[8581.6]|\n",
      "|        11832.0|     8606.4|        1.0|737236.7675818775| 19|[8606.4]|\n",
      "+---------------+-----------+-----------+-----------------+---+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vecAssembler = VectorAssembler(inputCols=['data_signal'], outputCol=\"features\")\n",
    "stream_df = vecAssembler.transform(signal_sdf)\n",
    "stream_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",withStd=True, withMean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute summary statistics by fitting the StandardScaler\n",
    "scalerModel = scaler.fit(stream_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DenseVector([7820.8872]), DenseVector([672.1962]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalerModel.mean,scalerModel.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------+-----------+-----------------+---+--------+--------------------+\n",
      "|data_signal_old|data_signal|data_switch|             time|idx|features|      scaledFeatures|\n",
      "+---------------+-----------+-----------+-----------------+---+--------+--------------------+\n",
      "|        10884.0|     8416.8|        1.0|737236.7675810185|  0|[8416.8]|[12.521345026274926]|\n",
      "|        11072.0|     8454.4|        1.0|737236.7675810637|  1|[8454.4]|[12.577281079524134]|\n",
      "|        11196.0|     8479.2|        1.0| 737236.767581109|  2|[8479.2]|[12.614175072092763]|\n",
      "|        11260.0|     8492.0|        1.0|737236.7675811541|  3|[8492.0]|[12.633217132773343]|\n",
      "|        11328.0|     8505.6|        1.0|737236.7675811993|  4|[8505.6]|[12.653449322246463]|\n",
      "|        11336.0|     8507.2|        1.0|737236.7675812446|  5|[8507.2]|[12.655829579831535]|\n",
      "|        11364.0|     8512.8|        1.0|737236.7675812898|  6|[8512.8]|[12.664160481379287]|\n",
      "|        11392.0|     8518.4|        1.0| 737236.767581335|  7|[8518.4]|[12.672491382927042]|\n",
      "|        11356.0|     8511.2|        1.0|737236.7675813802|  8|[8511.2]|[12.661780223794217]|\n",
      "|        11352.0|     8510.4|        1.0|737236.7675814254|  9|[8510.4]|[12.660590095001679]|\n",
      "|        11344.0|     8508.8|        1.0|737236.7675814707| 10|[8508.8]|[12.658209837416607]|\n",
      "|        11340.0|     8508.0|        1.0|737236.7675815158| 11|[8508.0]| [12.65701970862407]|\n",
      "|        11356.0|     8511.2|        1.0| 737236.767581561| 12|[8511.2]|[12.661780223794217]|\n",
      "|        11356.0|     8511.2|        1.0|737236.7675816063| 13|[8511.2]|[12.661780223794217]|\n",
      "|        11412.0|     8522.4|        1.0|737236.7675816515| 14|[8522.4]|[12.678442026889725]|\n",
      "|        11480.0|     8536.0|        1.0|737236.7675816966| 15|[8536.0]|[12.698674216362843]|\n",
      "|        11540.0|     8548.0|        1.0|737236.7675817419| 16|[8548.0]|[12.716526148250887]|\n",
      "|        11628.0|     8565.6|        1.0|737236.7675817871| 17|[8565.6]|[12.742708981686688]|\n",
      "|        11708.0|     8581.6|        1.0|737236.7675818322| 18|[8581.6]|[12.766511557537415]|\n",
      "|        11832.0|     8606.4|        1.0|737236.7675818775| 19|[8606.4]| [12.80340555010604]|\n",
      "+---------------+-----------+-----------+-----------------+---+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Normalize each feature to have unit standard deviation.\n",
    "scaledData = scalerModel.transform(stream_df)\n",
    "scaledData.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_df=scaledData.rdd.map(lambda x:[round(float(y),3) for y in x['scaledFeatures']]).toDF([\"signal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|signal|\n",
      "+------+\n",
      "|12.521|\n",
      "|12.577|\n",
      "|12.614|\n",
      "|12.633|\n",
      "|12.653|\n",
      "|12.656|\n",
      "|12.664|\n",
      "|12.672|\n",
      "|12.662|\n",
      "|12.661|\n",
      "|12.658|\n",
      "|12.657|\n",
      "|12.662|\n",
      "|12.662|\n",
      "|12.678|\n",
      "|12.699|\n",
      "|12.717|\n",
      "|12.743|\n",
      "|12.767|\n",
      "|12.803|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[signal: double]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_df.show()\n",
    "scale_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_df.agg(F.avg('signal')).show()\n",
    "#spark.createDataFrame(rdd,(\"signal\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fv=scaledData.first().scaledFeatures\n",
    "fv.toArray()\n",
    "pd.Series(np.arange(10))\n",
    "from pyspark.ml.linalg import Vectors\n",
    "psv=pd.Series([Vectors.dense([1.0,0.2]),Vectors.dense([2.0,12]),Vectors.dense([3.0,6])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av=psv[0].toArray()\n",
    "psv.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(psv.map(lambda x:x.toArray()).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
    "eDF.show()\n",
    "eDF.select(F.posexplode(eDF.intlist)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
    "eDF.show()\n",
    "eDF.select(F.explode(eDF.intlist).alias(\"anInt\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,50))\n",
    "plt.plot(signal['data_signal'],)\n",
    "plt.xticks(fontsize=60)\n",
    "plt.yticks(fontsize=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal=parse_mat_signal(rdp)\n",
    "psg=parse_mat_psg(p)\n",
    "\n",
    "psg_time=psg['time'].values\n",
    "psg_stage=psg['stage'].values\n",
    "psg_int=list(zip(psg_time[:-1],psg_time[1:],psg_stage[1:]))\n",
    "\n",
    "def make_signal_psg_list(psg,signal):\n",
    "    signal_list=[]\n",
    "    psg_list=[]\n",
    "    psg_time=psg['time'].values\n",
    "    psg_stage=psg['stage'].values\n",
    "    psg_int=list(zip(psg_time[:-1],psg_time[1:],psg_stage[1:]))\n",
    "    for it in psg_int:\n",
    "        signal_list.append(signal.query(f\"(time>{it[0]})and(time<{it[1]})\")['data_signal'].values)\n",
    "        psg_list.append(it[2])\n",
    "    return signal_list,psg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_list,psg_list=make_signal_psg_list(psg,signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=0\n",
    "\n",
    "named_featurelists=collections.defaultdict(list)\n",
    "\n",
    "for sig,label in zip(signal_list,psg_list):\n",
    "    c+=1\n",
    "    if c<10:\n",
    "        named_featurelists['signal_seq'].append(_bytes_feature(sig.tostring()))\n",
    "        named_featurelists['psg_seq'].append(_int64_feature(int(label)))\n",
    "        print(sig,label)\n",
    "    res=_make_named_featurelists({k:_feature_list(v) for k,v in named_featurelists.items()})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = tf.train.SequenceExample(feature_lists=res)\n",
    "filename='movie_ratings4.tfrecord'\n",
    "if tf.gfile.Exists(filename):\n",
    "    print(tf.gfile.Exists(filename))\n",
    "    tf.gfile.Remove(filename)\n",
    "    print(tf.gfile.Exists(filename))\n",
    "\n",
    "with tf.python_io.TFRecordWriter(filename) as writer:\n",
    "    writer.write(example.SerializeToString())\n",
    "    writer.write(example.SerializeToString())\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logFile = \"tmp.txt\"  # Should be some file on your system\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"SimpleApp\").getOrCreate()\n",
    "logData = spark.read.text(logFile).cache()\n",
    "\n",
    "numAs = logData.filter(logData.value.contains('a')).count()\n",
    "numBs = logData.filter(logData.value.contains('b')).count()\n",
    "\n",
    "print(\"Lines with a: %i, lines with b: %i\" % (numAs, numBs))\n",
    "\n",
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col=logData['value']\n",
    "col."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_features = {\n",
    "    'signal_seq': tf.FixedLenSequenceFeature([], dtype=tf.string),\n",
    "    'psg_seq': tf.FixedLenSequenceFeature([], dtype=tf.int64),\n",
    "}\n",
    "\n",
    "dataset = tf.data.TFRecordDataset(filename)\n",
    "#dataset = dataset.batch(1)\n",
    "#dataset=dataset.shuffle(200)\n",
    "dataset=dataset.repeat(4)\n",
    "iterator=dataset.make_one_shot_iterator()\n",
    "it=iterator.get_next()\n",
    "ps=tf.parse_single_sequence_example(it,sequence_features=sequence_features)\n",
    "ps_signal=tf.decode_raw(ps[1]['signal_seq'],tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps_signal2=ps_signal[:,0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfa=tf.convert_to_tensor(np.arange(144).reshape(-1,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.preprocessing.sequence.pad_sequences(tfs, maxlen=10, dtype='float32', padding='pre', truncating='pre', value=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    print(sess.run(ps_signal).shape)\n",
    "    print(\"-\"*30)\n",
    "    print(sess.run(ps_signal2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing.scale(np.arange(144).reshape(-1,12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    #print(sess.run(np.dot(tfa,tfa)))\n",
    "    print(sess.run(tf.reduce_sum(tfa)))\n",
    "    #print(sess.run(np.mean(tfa)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(tfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成tf.record文件\n",
    "# 本身项目标数据结构\n",
    "ar3=df3['data_signal'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tf_example(example_proto):\n",
    "\n",
    "    feature_spec = {}\n",
    "\n",
    "    for feature_name in NUMERIC_FEATURE_NAMES:\n",
    "        feature_spec[feature_name] = tf.FixedLenFeature(shape=(1), dtype=tf.float32)\n",
    "    \n",
    "    for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "        feature_spec[feature_name] = tf.FixedLenFeature(shape=(1), dtype=tf.string)\n",
    "    \n",
    "    feature_spec[TARGET_NAME] = tf.FixedLenFeature(shape=(1), dtype=tf.float32)\n",
    "\n",
    "    parsed_features = tf.parse_example(serialized=example_proto, features=feature_spec)\n",
    "    \n",
    "    target = parsed_features.pop(TARGET_NAME)\n",
    "    \n",
    "    return parsed_features, target\n",
    "\n",
    "\n",
    "def process_features(features):\n",
    "    \n",
    "    # example of clipping\n",
    "    features['x'] = tf.clip_by_value(features['x'], clip_value_min=-3, clip_value_max=3)\n",
    "    features['y'] = tf.clip_by_value(features['y'], clip_value_min=-3, clip_value_max=3)\n",
    "    \n",
    "    # example of polynomial expansion\n",
    "    features[\"x_2\"] = tf.square(features['x'])\n",
    "    features[\"y_2\"] = tf.square(features['y'])\n",
    "    \n",
    "    # example of nonlinearity\n",
    "    features[\"xy\"] = features['x'] * features['y']\n",
    "    \n",
    "    # example of custom logic\n",
    "    features['dist_xy'] =  tf.sqrt(tf.squared_difference(features['x'],features['y']))\n",
    "    features[\"sin_x\"] = tf.sin(features['x'])\n",
    "    features[\"cos_y\"] = tf.sin(features['y'])    \n",
    "    return features\n",
    "\n",
    "def tfrecods_input_fn(files_name_pattern, mode=tf.estimator.ModeKeys.EVAL, \n",
    "                 num_epochs=None, \n",
    "                 batch_size=200):\n",
    "    \n",
    "    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"* data input_fn:\")\n",
    "    print(\"================\")\n",
    "    print(\"Input file(s): {}\".format(files_name_pattern))\n",
    "    print(\"Batch size: {}\".format(batch_size))\n",
    "    print(\"Epoch Count: {}\".format(num_epochs))\n",
    "    print(\"Mode: {}\".format(mode))\n",
    "    print(\"Shuffle: {}\".format(shuffle))\n",
    "    print(\"================\")\n",
    "    print(\"\")\n",
    "\n",
    "    file_names = tf.matching_files(files_name_pattern)\n",
    "    dataset = data.TFRecordDataset(filenames=file_names)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda tf_example: parse_tf_example(tf_example))\n",
    "    \n",
    "    if PROCESS_FEATURES:\n",
    "        dataset = dataset.map(lambda features, target: (process_features(features), target))\n",
    "        \n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    features, target = iterator.get_next()\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_name_list = tf.train.BytesList(value=[b'The Shawshank Redemption', b'Fight Club'])\n",
    "movie_rating_list = tf.train.FloatList(value=[9.0, 9.7, 5.5])\n",
    "\n",
    "movie_names = tf.train.Feature(bytes_list=movie_name_list)\n",
    "movie_ratings = tf.train.Feature(float_list=movie_rating_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_dict = { 'Movie Names': movie_names, 'Movie Ratings': movie_ratings}\n",
    "movies = tf.train.Features(feature=movie_dict)\n",
    "example = tf.train.Example(features=movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.parse_example(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.python_io.TFRecordWriter('movie_ratings.tfrecord') as writer:\n",
    "    writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psg_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.query(\"(time>737201.8932291215)and(time<737201.8935763418)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "30*256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=datetime.fromordinal(int(matlab_datenum) - 366) + timedelta(days=matlab_datenum%1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(100,30))\n",
    "plt.plot(df['time'],[1]*len(df))\n",
    "plt.plot(df2['time'],[2]*len(df2))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.merge(df2,on='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2[\"time\"]==737201.9848952909]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df2.query(\"time>737201.9 and time<737201.99\")['time']:print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td=parse_mat_time(df['time'][0])-parse_mat_time(df['time'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_mat_time(df['time'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_mat_time(df['time'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td.seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "td.total_seconds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1=parse_mat_time(df['time'][0])\n",
    "t2=parse_mat_time(df['time'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1.second,t2.second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
