{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scipy.io as sio \n",
    "import path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"msg10\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\environment\\python\\anaconda352\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据说明：\n",
    "1. 对于PSG数据\n",
    "   - stage 睡眠所处阶段\n",
    "   - 3 -> 醒来， 2 -> 浅睡， 1 -> 深睡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root=r\"E:\\datasources\\MS10Data\"\n",
    "p = r\"E:\\datasources\\MS10Data\\1\\psg\\psg.mat\"\n",
    "rdp=r\"E:\\datasources\\MS10Data\\1\\raw\\rawMat.mat\"\n",
    "save_data=r\"E:\\datasources\\MS10DataTF\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "psg_data_paths=[os.path.join(data_root,f\"{i}\\\\psg\\\\psg.mat\") for i in range(1,33)]\n",
    "raw_data_paths=[os.path.join(data_root,f\"{i}\\\\raw\\\\rawMat.mat\") for i in range(1,33)]\n",
    "psg_raw_data_paths=list(zip(psg_data_paths,raw_data_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_mat_psg(fp):\n",
    "    psg1=sio.loadmat(fp)\n",
    "    ti=np.apply_along_axis(lambda row:row[0],0,psg1[\"psgStage\"]['Time']).flatten()\n",
    "    st=np.apply_along_axis(lambda row:row[0],0,psg1[\"psgStage\"]['stage']).flatten()\n",
    "    st=np.apply_along_axis(lambda x:int(x),1,st.reshape(-1,1))\n",
    "    print(st.shape)\n",
    "    return pd.DataFrame(np.vstack([ti,st]).transpose(),columns=['time','stage'])\n",
    "\n",
    "def parse_mat_signal(fp):\n",
    "    rs=sio.loadmat(fp)\n",
    "    r1=rs['rawData']['dataSignalOld'][0,0]\n",
    "    r2=rs['rawData']['dataSignal'][0,0]\n",
    "    r3=rs['rawData']['dataSwitch'][0,0]\n",
    "    r4=rs['rawData']['Time'][0,0]\n",
    "    return pd.DataFrame(np.hstack([r1,r2,r3,r4]),columns=['data_signal_old','data_signal','data_switch','time'])\n",
    "\n",
    "def parse_mat_time(matlab_datenum):\n",
    "    return datetime.fromordinal(int(matlab_datenum)) + timedelta(days=matlab_datenum%1) - timedelta(days = 366)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据解析生成tf-record\n",
    "# 定义特征对象\n",
    "def _int64_feature(value):\n",
    "    \"\"\"生成一个对应类型的特征对象\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"生成一个对应类型的特征对象\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"生成一个对应类型的特征对象\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "def _make_named_features(named_features):\n",
    "    \"\"\"生成一个命名的特征，给特征对象赋予一个schema\"\"\"\n",
    "    return tf.train.Features(feature=named_features)\n",
    "\n",
    "# 定义序列record，多了一层包裹的抽象\n",
    "def _feature_list(feature_list):\n",
    "    \"\"\"生成一个特征对象的序列\"\"\"\n",
    "    return tf.train.FeatureList(feature=feature_list)\n",
    "\n",
    "def _make_named_featurelists(named_featurelists):\n",
    "    \"\"\"生成一个命名的特征，给特征对象的序列赋予一个schema\"\"\"\n",
    "    return tf.train.FeatureLists(feature_list=named_featurelists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'signal' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-c69bc92b56bf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'data_signal'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0myticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfontsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'signal' is not defined"
     ],
     "output_type": "error"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 7200x3600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(100,50))\n",
    "plt.plot(signal['data_signal'],)\n",
    "plt.xticks(fontsize=60)\n",
    "plt.yticks(fontsize=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1116,)\n"
     ]
    }
   ],
   "source": [
    "signal=parse_mat_signal(rdp)\n",
    "psg=parse_mat_psg(p)\n",
    "\n",
    "psg_time=psg['time'].values\n",
    "psg_stage=psg['stage'].values\n",
    "psg_int=list(zip(psg_time[:-1],psg_time[1:],psg_stage[1:]))\n",
    "\n",
    "def make_signal_psg_list(psg,signal):\n",
    "    signal_list=[]\n",
    "    psg_list=[]\n",
    "    psg_time=psg['time'].values\n",
    "    psg_stage=psg['stage'].values\n",
    "    psg_int=list(zip(psg_time[:-1],psg_time[1:],psg_stage[1:]))\n",
    "    for it in psg_int:\n",
    "        signal_list.append(signal.query(f\"(time>{it[0]})and(time<{it[1]})\")['data_signal'].values)\n",
    "        psg_list.append(it[2])\n",
    "    return signal_list,psg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_list,psg_list=make_signal_psg_list(psg,signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "|   _1| _2|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "+-----+---+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\environment\\python\\anaconda352\\lib\\site-packages\\pyspark\\sql\\session.py:340: UserWarning: inferring schema from dict is deprecated,please use pyspark.sql.Row instead\n",
      "  warnings.warn(\"inferring schema from dict is deprecated,\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|age| name|\n",
      "+---+-----+\n",
      "|  1|Alice|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 从字典列表创建df\n",
    "\n",
    "l = [('Alice', 1)]\n",
    "spark.createDataFrame(l).show()\n",
    "spark.createDataFrame(l, ['name', 'age']).show()\n",
    "d = [{'name': 'Alice', 'age': 1}]\n",
    "rs=spark.createDataFrame(d).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# 从rdd创建df\n",
    "rdd = spark.sparkContext.parallelize(l)\n",
    "spark.createDataFrame(rdd).collect()\n",
    "df = spark.createDataFrame(rdd, ['name', 'age'])\n",
    "df.collect()\n",
    "\n",
    "Person = Row('name', 'age')\n",
    "person = rdd.map(lambda r: Person(*r))\n",
    "df2 = spark.createDataFrame(person)\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Alice', age=1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "# 指明schema 结合rdd创建df\n",
    "schema = StructType([StructField(\"name\", StringType(), True),StructField(\"age\", IntegerType(), True)])\n",
    "df3 = spark.createDataFrame(rdd, schema)\n",
    "df3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "|    a|  b|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 从 pandas 创建\n",
    "spark.createDataFrame(df.toPandas()).show()\n",
    "spark.createDataFrame(rdd, \"a: string, b: int\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    name  age\n",
       "0  Alice    1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "|Alice|  1|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(f1='Alice', f2=1)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"table1\")\n",
    "df2 = spark.sql(\"SELECT name AS f1, age as f2 from table1\")\n",
    "df2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"table1\")\n",
    "df2 = spark.table(\"table1\")\n",
    "sorted(df.collect()) == sorted(df2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+\n",
      "|stringLengthString(test)|\n",
      "+------------------------+\n",
      "|                       4|\n",
      "+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "strlen = spark.udf.register(\"stringLengthString\", lambda x: len(x))\n",
    "spark.sql(\"SELECT stringLengthString('test')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.<lambda>(x)>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT 'foo' AS text\").select(strlen(\"text\")).show()\n",
    "df.select(strlen(\"name\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+\n",
      "|stringLengthInt(test)|\n",
      "+---------------------+\n",
      "|         testtesttest|\n",
      "+---------------------+\n",
      "\n",
      "+----------+\n",
      "|slen(test)|\n",
      "+----------+\n",
      "|         4|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import udf\n",
    "import random\n",
    "\n",
    "_ = spark.udf.register(\"stringLengthInt\", lambda x: x*3)\n",
    "spark.sql(\"SELECT stringLengthInt('test')\").show()\n",
    "\n",
    "slen = udf(lambda s: len(s), IntegerType())\n",
    "_= spark.udf.register(\"slen\", slen)\n",
    "spark.sql(\"SELECT slen('test')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_udf = udf(lambda: random.randint(0, 100), IntegerType()).asNondeterministic()\n",
    "new_random_udf = spark.udf.register(\"random_udf\", random_udf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|<lambda>()|\n",
      "+----------+\n",
      "|        39|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT 'random_udf' as text\").select(new_random_udf()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "@pandas_udf(\"integer\", PandasUDFType.SCALAR)  # doctest: +SKIP\n",
    "def add_one(x):\n",
    "    return x + 1\n",
    "_ = spark.udf.register(\"add_one\", add_one)  # doctest: +SKIP\n",
    "spark.sql(\"SELECT add_one(id) FROM range(3)\").show()  # doctest: +SKIP\n",
    "\n",
    "# @pandas_udf(\"integer\", PandasUDFType.GROUPED_AGG)  # doctest: +SKIP\n",
    "# def sum_udf(v):\n",
    "#     return v.sum()\n",
    "\n",
    "# _ = spark.udf.register(\"sum_udf\", sum_udf)  # doctest: +SKIP\n",
    "# q = \"SELECT sum_udf(v1) FROM VALUES (3, 0), (2, 0), (1, 1) tbl(v1, v2) GROUP BY v2\"\n",
    "# spark.sql(q).show()  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "spark.udf.registerJavaFunction(\"javaStringLength\", \"test.org.apache.spark.sql.JavaStringLength\", IntegerType())\n",
    "spark.sql(\"SELECT javaStringLength('test')\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|max(age)|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|min(age)|\n",
      "+--------+\n",
      "|       1|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.agg({\"age\": \"max\"}).show()\n",
    "from pyspark.sql import functions as F\n",
    "df.agg(F.min(df.age)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-309e615779b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_as1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"df_as1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf_as2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"df_as2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mjoined_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf_as1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_as2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"df_as1.name\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"df_as2.name\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inner'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mjoined_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"df_as1.name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"df_as2.name\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"df_as2.age\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "df_as1 = df.alias(\"df_as1\")\n",
    "df_as2 = df.alias(\"df_as2\")\n",
    "joined_df = df_as1.join(df_as2, col(\"df_as1.name\") == col(\"df_as2.name\"), 'inner')\n",
    "joined_df.select(\"df_as1.name\", \"df_as2.name\", \"df_as2.age\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([(\"a\", 11), (\"a\", 5), (\"a\", 7), (\"a\", 2), (\"b\",  3), (\"c\", 4)], [\"C1\", \"C2\"])\n",
    "df2 = spark.createDataFrame([(\"a\", 1), (\"b\", 3)], [\"C1\", \"C2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a| 11|\n",
      "|  a|  5|\n",
      "|  a|  7|\n",
      "|  a|  2|\n",
      "|  b|  3|\n",
      "|  c|  4|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a| 11|\n",
      "|  a|  5|\n",
      "|  a|  7|\n",
      "|  a|  2|\n",
      "|  b|  3|\n",
      "|  c|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'col' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-7a54de5b9e27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C2>1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C2>1\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C2\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'col' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "df1.filter(\"C2>1\").show()\n",
    "df1.where(\"C2>1\").show()\n",
    "df1.select(col(\"C2\")>1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a| 11|\n",
      "|  a|  7|\n",
      "|  a|  5|\n",
      "|  c|  4|\n",
      "|  b|  3|\n",
      "|  a|  2|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a| 11|\n",
      "|  a|  7|\n",
      "|  a|  5|\n",
      "|  c|  4|\n",
      "|  b|  3|\n",
      "|  a|  2|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a| 11|\n",
      "|  a|  7|\n",
      "|  a|  5|\n",
      "|  c|  4|\n",
      "|  b|  3|\n",
      "|  a|  2|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a|  2|\n",
      "|  b|  3|\n",
      "|  c|  4|\n",
      "|  a|  5|\n",
      "|  a|  7|\n",
      "|  a| 11|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a| 11|\n",
      "|  a|  7|\n",
      "|  a|  5|\n",
      "|  c|  4|\n",
      "|  b|  3|\n",
      "|  a|  2|\n",
      "+---+---+\n",
      "\n",
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a| 11|\n",
      "|  a|  7|\n",
      "|  a|  5|\n",
      "|  c|  4|\n",
      "|  b|  3|\n",
      "|  a|  2|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.sort(df1[\"C2\"].desc()).show()\n",
    "df1.sort(\"C2\", ascending=False).show()\n",
    "df1.orderBy(df1[\"C2\"].desc()).show()\n",
    "from pyspark.sql.functions import *\n",
    "df1.sort(asc(\"C2\")).show()\n",
    "df1.orderBy(desc(\"C2\"), \"C1\").show()\n",
    "df1.orderBy([\"C2\", \"C1\"], ascending=[0, 1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a| 11|\n",
      "|  a| 20|\n",
      "|  a|  7|\n",
      "|  a|  2|\n",
      "|  b|  3|\n",
      "|  c|  4|\n",
      "+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.na.replace(5,20,'C2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|key|count|\n",
      "+---+-----+\n",
      "|  0|    3|\n",
      "|  1|    7|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = spark.range(0, 100).select((col(\"id\") % 3).alias(\"key\"))\n",
    "sampled = dataset.sampleBy(\"key\", fractions={0: 0.1, 1: 0.2}, seed=0)\n",
    "sampled.groupBy(\"key\").count().orderBy(\"key\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "| C1|count(1)|\n",
      "+---+--------+\n",
      "|  c|       1|\n",
      "|  b|       1|\n",
      "|  a|       4|\n",
      "+---+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gdf = df1.groupBy(df1.C1)\n",
    "gdf.agg({\"*\": \"count\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|                  v|\n",
      "+---+-------------------+\n",
      "|  1|-0.7071067811865475|\n",
      "|  1| 0.7071067811865475|\n",
      "|  2|-0.8320502943378437|\n",
      "|  2|-0.2773500981126146|\n",
      "|  2| 1.1094003924504583|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "df = spark.createDataFrame([(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],(\"id\", \"v\"))\n",
    "@pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\n",
    "def normalize(pdf):\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=(v - v.mean()) / v.std())\n",
    "df.groupby(\"id\").apply(normalize).show()  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------+\n",
      "|     l|             d|\n",
      "+------+--------------+\n",
      "|[1, 2]|[key -> value]|\n",
      "+------+--------------+\n",
      "\n",
      "+----+------+\n",
      "|l[0]|d[key]|\n",
      "+----+------+\n",
      "|   1| value|\n",
      "+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([([1, 2], {\"key\": \"value\"})], [\"l\", \"d\"])\n",
    "df.show()\n",
    "df.select(df.l.getItem(0), df.d.getItem(\"key\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+\n",
      "| C1| C2|\n",
      "+---+---+\n",
      "|  a| 11|\n",
      "|  a|  5|\n",
      "|  a|  7|\n",
      "|  a|  2|\n",
      "|  b|  3|\n",
      "|  c|  4|\n",
      "+---+---+\n",
      "\n",
      "+---+\n",
      "|c1_|\n",
      "+---+\n",
      "|  1|\n",
      "|  1|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n",
      "+---+\n",
      "|c2_|\n",
      "+---+\n",
      "|  4|\n",
      "|  3|\n",
      "|  2|\n",
      "|  2|\n",
      "|  5|\n",
      "|  7|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql import functions as F\n",
    "window = Window.partitionBy(\"C1\").orderBy(\"C2\")\n",
    "df1.show()\n",
    "window2 = Window.partitionBy(\"C1\").orderBy(\"C2\").rowsBetween(-1,1)\n",
    "df1.select(F.dense_rank().over(window).alias(\"c1_\")).show()\n",
    "df1.select(F.min(\"C2\").over(window2).alias(\"c2_\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.window.WindowSpec at 0x1e21cd7beb8>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window.rowsBetween(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.window.WindowSpec at 0x1e21cd5ada0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p=r\"E:\\GOOD_PROJECTS\\spark-master\\python\\test_support\\sql\\ages.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders = spark.createDataFrame([(\"o1\", \"s1\", \"2017-05-01\", 100), (\"o2\", \"s1\", \"2017-05-02\", 200), (\"o3\", \"s2\", \"2017-05-01\", 300)],\n",
    "                              [\"order_id\", \"seller_id\", \"order_date\", \"price\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----------+-----+\n",
      "|order_id|seller_id|order_date|price|\n",
      "+--------+---------+----------+-----+\n",
      "|      o1|       s1|2017-05-01|  100|\n",
      "|      o2|       s1|2017-05-02|  200|\n",
      "|      o3|       s2|2017-05-01|  300|\n",
      "+--------+---------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----------+-----+----+\n",
      "|order_id|seller_id|order_date|price|rank|\n",
      "+--------+---------+----------+-----+----+\n",
      "|      o3|       s2|2017-05-01|  300|   1|\n",
      "|      o1|       s1|2017-05-01|  100|   1|\n",
      "|      o2|       s1|2017-05-02|  200|   2|\n",
      "+--------+---------+----------+-----+----+\n",
      "\n",
      "+--------+---------+----------+-----+--------------+\n",
      "|order_id|seller_id|order_date|price|cumulative_sum|\n",
      "+--------+---------+----------+-----+--------------+\n",
      "|      o3|       s2|2017-05-01|  300|           300|\n",
      "|      o1|       s1|2017-05-01|  100|           100|\n",
      "|      o2|       s1|2017-05-02|  200|           300|\n",
      "+--------+---------+----------+-----+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rankSpec = Window.partitionBy(\"seller_id\").orderBy(\"order_date\")\n",
    "\n",
    "shopOrderRank = orders.withColumn(\"rank\", F.dense_rank().over(rankSpec)).show()  \n",
    "\n",
    "sumSpec = Window.partitionBy(\"seller_id\").orderBy(\"order_date\").rowsBetween(-1, 0)\n",
    "\n",
    "orders.withColumn(\"cumulative_sum\", F.sum(\"price\").over(sumSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[(\"站点1\", \"2017-01-01\", 50),(\"站点1\", \"2017-01-02\", 45), (\"站点1\", \"2017-01-03\", 55), \n",
    " (\"站点2\", \"2017-01-01\", 25), (\"站点2\", \"2017-01-02\", 29),(\"站点2\", \"2017-01-03\", 27)]\n",
    "\n",
    "schema=(\"site\", \"date\", \"user_cnt\")\n",
    "df_w=spark.createDataFrame(data,schema)\n",
    "\n",
    "#wSpec = window.partitionBy(\"site\").orderBy(\"date\").rowsBetween(-1, 1)\n",
    "wSpec = window.partitionBy().orderBy(\"date\").rowsBetween(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+--------+\n",
      "|site|      date|user_cnt|\n",
      "+----+----------+--------+\n",
      "| 站点1|2017-01-01|      50|\n",
      "| 站点1|2017-01-02|      45|\n",
      "| 站点1|2017-01-03|      55|\n",
      "| 站点2|2017-01-01|      25|\n",
      "| 站点2|2017-01-02|      29|\n",
      "| 站点2|2017-01-03|      27|\n",
      "+----+----------+--------+\n",
      "\n",
      "+----+----------+--------+---------+\n",
      "|site|      date|user_cnt|movingAvg|\n",
      "+----+----------+--------+---------+\n",
      "| 站点1|2017-01-01|      50|     37.5|\n",
      "| 站点2|2017-01-01|      25|     40.0|\n",
      "| 站点1|2017-01-02|      45|     33.0|\n",
      "| 站点2|2017-01-02|      29|     43.0|\n",
      "| 站点1|2017-01-03|      55|     37.0|\n",
      "| 站点2|2017-01-03|      27|     41.0|\n",
      "+----+----------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_w.show()\n",
    "df_w.withColumn(\"movingAvg\",F.avg(df_w[\"user_cnt\"]).over(wSpec)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60, 55, 65]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_w=spark.sparkContext.parallelize([50,45,55])\n",
    "rdd_w.map(lambda x:x+10).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'arrays_zip'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-348ef053cd27>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marrays_zip\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdf0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'vals1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vals2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdf0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays_zip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvals1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvals2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'zipped'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'arrays_zip'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from pyspark.sql.functions import arrays_zip\n",
    "df0 = spark.createDataFrame([(([1, 2, 3], [2, 3, 4]))], ['vals1', 'vals2'])\n",
    "df0.select(arrays_zip(df0.vals1, df.vals2).alias('zipped')).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+----------+\n",
      "| id|  an_array|     a_map|\n",
      "+---+----------+----------+\n",
      "|  1|[foo, bar]|[x -> 1.0]|\n",
      "|  2|        []|        []|\n",
      "|  3|      null|      null|\n",
      "+---+----------+----------+\n",
      "\n",
      "+---+----------+----+-----+\n",
      "| id|  an_array| key|value|\n",
      "+---+----------+----+-----+\n",
      "|  1|[foo, bar]|   x|  1.0|\n",
      "|  2|        []|null| null|\n",
      "|  3|      null|null| null|\n",
      "+---+----------+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],(\"id\", \"an_array\", \"a_map\"))\n",
    "df.show()\n",
    "df.select(\"id\", \"an_array\", explode_outer(\"a_map\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|            data|\n",
      "+----------------+\n",
      "|[1 -> a, 2 -> b]|\n",
      "+----------------+\n",
      "\n",
      "+------+\n",
      "|  keys|\n",
      "+------+\n",
      "|[1, 2]|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_keys\n",
    "df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
    "df.show()\n",
    "df.select(map_keys(\"data\").alias(\"keys\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(data={1: 'a', 2: 'b'})]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(data={1: 'a', 2: 'b'})]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------------+------------+\n",
      "|slen(name)|  to_upper(name)|add_one(age)|\n",
      "+----------+----------------+------------+\n",
      "|         8|Series'>JOHN DOE|          22|\n",
      "+----------+----------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import IntegerType, StringType\n",
    "slen = pandas_udf(lambda s: s.str.len(), IntegerType())  # doctest: +SKIP\n",
    "@pandas_udf(StringType())  # doctest: +SKIP\n",
    "def to_upper(s):\n",
    "    # 输入和输出是等长是series\n",
    "    #print(s1.values,s.__class__)\n",
    "    return str(type(s))[-8:]+s.str.upper()\n",
    "@pandas_udf(\"integer\", PandasUDFType.SCALAR)  # doctest: +SKIP\n",
    "def add_one(x):\n",
    "    print(x.values,x.__class__)\n",
    "    return x + 1\n",
    "\n",
    "df = spark.createDataFrame([(1, \"John Doe\", 21)],(\"id\", \"name\", \"age\"))  # doctest: +SKIP\n",
    "df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()  # doctest: +SKIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------------------+\n",
      "| id|                  v|\n",
      "+---+-------------------+\n",
      "|  1|-0.7071067811865475|\n",
      "|  1| 0.7071067811865475|\n",
      "|  2|-0.8320502943378437|\n",
      "|  2|-0.2773500981126146|\n",
      "|  2| 1.1094003924504583|\n",
      "+---+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],(\"id\", \"v\"))  # doctest: +SKIP\n",
    "@pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\n",
    "def normalize(pdf):\n",
    "    v = pdf.v\n",
    "    return pdf.assign(v=(v - v.mean()) / v.std())\n",
    "df.groupby(\"id\").apply(normalize).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['a33', 'b33', 'c33', 'd33', 'e33'], dtype=object)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "must be str, not tuple",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-146-4e7b8801f11d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpdf\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"v\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"key\"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: must be str, not tuple"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "pdf= pd.DataFrame([(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],columns=(\"id\", \"v\"))\n",
    "pd.DataFrame([\"key\" + (pdf.v.mean(),)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sequence' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-9bdba846a7b5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'C1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'C2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'C2'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreateDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'C1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'C2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'C3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdf2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C1'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'C2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'C3'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malias\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sequence' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "f1 = spark.createDataFrame([(-2, 2)], ('C1', 'C2'))\n",
    "df1.select(sequence('C1', 'C2').alias('r')).collect()\n",
    "df2 = spark.createDataFrame([(4, -4, -2)], ('C1', 'C2', 'C3'))\n",
    "df2.show()\n",
    "df2.select(F.sequence('C1', 'C2', 'C3').alias('r')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[myf(abc): string]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myf=udf(len)\n",
    "strlen = spark.udf.register(\"myf\", lambda x: len(x))\n",
    "spark.sql(\"Select myf('abc')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply_func(a, b):\n",
    "    return a * b\n",
    "\n",
    "multiply = F.pandas_udf(multiply_func, returnType=sp_types.FloatType())\n",
    "\n",
    "x = pd.Series([1, 2, 3])\n",
    "print(multiply_func(x, x))\n",
    "tmp_df=pd.DataFrame(np.arange(12).reshape(-1,3),columns=list('abc'))\n",
    "tmp_df=tmp_df.assign(gp=[1,1,2,2])\n",
    "tmp_sdf=spark.createDataFrame(tmp_df)\n",
    "tmp_sdf.show()\n",
    "tmp_sdfg=tmp_sdf.groupBy(\"gp\")\n",
    "\n",
    "# 使用框架内置的聚合函数实现简单的用户自定义的聚合函数\n",
    "def my_af(col_name):\n",
    "    coln=F.max(F.col(col_name))+F.max(F.col(col_name))+10\n",
    "    return coln.alias(\"c2\")\n",
    "\n",
    "tmp_sdfg.agg(F.max(F.col(\"c\"))*F.min(F.col(\"c\"))).show()\n",
    "tmp_sdfg.agg(my_af('c')).show()\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "df = spark.createDataFrame([(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)],(\"id\", \"v\"))  # doctest: +SKIP\n",
    "#@pandas_udf(\"id long, v double, v1 double, v2 double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\n",
    "@pandas_udf(\"id long, v double\", PandasUDFType.GROUPED_MAP)  # doctest: +SKIP\n",
    "def normalize(pdf):\n",
    "    v = pdf.v\n",
    "    print(pdf)\n",
    "    print(v.__class__,pdf.columns)\n",
    "    #res=pdf.assign(v1=(v - v.mean()) / v.std(),v2=v.mean())\n",
    "    res = pd.DataFrame(data=[pdf.id[0],v.mean()]).T\n",
    "    print(res)\n",
    "    return res\n",
    "df.groupby(\"id\").apply(normalize).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df=pd.DataFrame(np.arange(12).reshape(-1,3),columns=list('abc'))\n",
    "tmp_df=tmp_df.assign(gp=[1,1,2,2])\n",
    "tmp_sdf=spark.createDataFrame(tmp_df)\n",
    "tmp_sdf.show()\n",
    "tmp_sdfg=tmp_sdf.groupBy(\"gp\")\n",
    "# 使用框架内置的聚合函数实现简单的用户自定义的聚合函数\n",
    "\n",
    "mean_af=F.pandas_udf(lambda s:pd.Series([s.mean()]*s.size),returnType=sp_types.FloatType(),functionType=F.PandasUDFType.SCALAR)\n",
    "\n",
    "def my_af(col_name):\n",
    "    coln=F.max(F.col(col_name))+F.max(F.col(col_name))+10\n",
    "    return coln.alias(\"c2\")\n",
    "tmp_sdfg.agg(F.max(F.col(\"c\"))*F.min(F.col(\"c\"))).show()\n",
    "tmp_sdfg.agg(my_af('c')).show()\n",
    "tmp_sdfg.agg(F.collect_list('c').alias('value_list')).show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
